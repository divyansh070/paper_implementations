{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBnFgmv2oSu7zrU26ofR8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyansh070/paper_implemtaions/blob/main/transformer_testing/bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning from https://youtu.be/UU1WVnMk4E8?feature=shared vedio **Create a Large Language Model from Scratch with Python – Tutorial**"
      ],
      "metadata": {
        "id": "reUpIMPwXlkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "jRdRoRnwcmIn"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "RPOHepfHbFS_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "max_iters = 1000\n",
        "# eval_interval = 2500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZqRsgH8f1bt",
        "outputId": "60dac832-c9f6-4a28-8b89-271735aa0ebc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/wizard_of_oz.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "# print(len(text))\n",
        "chars=sorted(set(text))\n",
        "print(len(chars))\n",
        "print(chars)\n",
        "vocab_size=len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8w-V0wdYITy",
        "outputId": "2a76a6b8-a8ee-4529-fde1-824681401bd9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int={ch:i for i,ch in enumerate(chars)}\n",
        "int_to_string={i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [string_to_int[c] for c in s ]\n",
        "decode = lambda l: \"\".join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "LewKG8ptZqta"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr5XSewLad_d",
        "outputId": "23274c1f-8b25-403b-ce00-150e165c72a0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([91, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
            "        59, 62, 75, 64,  1, 62, 30, 72, 72, 68,  1, 72, 63,  1, 32, 72, 75, 72,\n",
            "        77, 65, 82,  1, 58, 71, 61,  1, 77, 65, 62,  1, 51, 66, 83, 58, 75, 61,\n",
            "         1, 66, 71,  1, 43, 83,  0,  1,  1,  1,  1,  0, 48, 65, 66, 76,  1, 62,\n",
            "        59, 72, 72, 68,  1, 66, 76,  1, 63, 72, 75,  1, 77, 65, 62,  1, 78, 76,\n",
            "        62,  1, 72, 63,  1, 58, 71, 82, 72, 71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "# print(x.shape)\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EWP4BrNar2Z",
        "outputId": "92f6cf76-0334-4697-ae61-f004c8462959"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[71, 64, 58, 59, 72, 72, 76,  1],\n",
            "        [58, 71, 61,  1, 58,  1, 71, 62],\n",
            "        [ 3, 51, 65, 58, 77,  1, 64, 72],\n",
            "        [77, 12,  3,  1, 60, 72, 71, 77]])\n",
            "targets:\n",
            "tensor([[64, 58, 59, 72, 72, 76,  1, 60],\n",
            "        [71, 61,  1, 58,  1, 71, 62, 66],\n",
            "        [51, 65, 58, 77,  1, 64, 72, 72],\n",
            "        [12,  3,  1, 60, 72, 71, 77, 66]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-t1a4d5E2nLs"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "EYFpgA_FzCYp"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "yzg4zGONcgpn"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "token = torch.tensor([1, 2, 3, 4, 5, 6])  # token indices\n",
        "embedding = nn.Embedding(num_embeddings=7, embedding_dim=3)  # vocab size = 10, dim = 3\n",
        "\n",
        "output = embedding(token)  # shape: (6, 3)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x13MpBh08rEL",
        "outputId": "4ff49a49-08da-4ea6-e3da-73ea768f0826"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1561,  0.0089,  0.2885],\n",
            "        [-1.5089,  1.7304,  0.0097],\n",
            "        [ 0.5964,  0.5623,  0.1016],\n",
            "        [-0.3619,  0.5524,  1.1342],\n",
            "        [-0.9880, -0.6850,  0.5136],\n",
            "        [ 2.1052, -0.7911,  0.0927]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        logits = self.token_embedding_table(index)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(index)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "zyNoLT-KfMut"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9dzZyslhZDL",
        "outputId": "05a5ab71-23a7-48dc-f4b5-36043d54eb1f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "!O2/JvzUx[rwu‘RiGh0 Ivp%3#—UK4jiUC”™b1”3SZ﻿on\n",
            "S9OT ,p‘/#VO.p[?ApdjMDv™.K$CGm?’kTWP1#6_:!Xub5WlQG!f2b7yK)\"9P!/gUn\n",
            "P*e)Gsv]tMlI6bi﻿oaM.g!•™xy3,•A$oLT‘vk4lUEQR%ElXt.“y—xbReO5G[?﻿mA0‘F8]g'Zwf—Lp/5]o‘$!2.1MzKtyz“M1‘QGA—J'f—xU_Vsmfq•AEgiVyGu22ZkspoQu1hMw\"LjR#K0g1w&?—#_pP!erG tUM#FRXS(odAp™xF%Nt-e#l*uIR•rSAs$IW,“MZ7rhR 1R:H7/G\"•$DJxJ—jP!1*b‘u)]gbUgu‘—])_d2&Lp]VB[1.A5“6Cal0&Ey)﻿R•l59bUZh:ON™ lXIOG22J't‘—uZJiqGK_11HC%usPxhd8_y]2SWrg'yB[\n",
            "noQC(-K$XUxSTss:oQ8wc*']nOO2_&r1,M5lFInO,Mh)xc[sQm5)L?5JH22EN‘P?A4’L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkgknIw4B5Xr",
        "outputId": "f73fd5fc-9409-4576-fc22-bf121a1dde06"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.910, val loss: 4.940\n",
            "step: 250, train loss: 4.856, val loss: 4.908\n",
            "step: 500, train loss: 4.789, val loss: 4.860\n",
            "step: 750, train loss: 4.717, val loss: 4.794\n",
            "4.462522983551025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0eumhYUB7Tf",
        "outputId": "4a33ad7f-c437-4638-897b-8d55d92b200b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "h—&n”5_wdf—tn﻿ijvxShuD”(mo&Los”k/:OMIdZP7dAn6,d?tP*a4yh6f—tVaFQ$IoRIFan’#“tV:oI?nzk;Xy﻿uf-Eu)•”g'ndA0c—’srAL3, IA3h﻿nq;PooQG!”™Lo.“NZ’9u6)Z.:-Q’K/Sc[5O&M[4JVBBpel8]vknXhhYjUjOf\n",
            "&‘zHD•[A&Z(\"a••AAAm_n6Xck—™b‘\n",
            "ZzLj\"’x2UnD4xA5U\n",
            "”jl5PV[h7KP™LYY—”(FDnOTYdApnl;/: Mh?L\"Pk;”(Zv”&_d2 x_ra4z\"’•z,I*H[UZb“qjZKg%e”wOfRR7‘—%D ,O.﻿J%'PWF,xHneZ2a7;gr_rS;Ygz•7X8da.﻿i/jZpksrVDmY5Z?!9.OR:OAr.pK)].#qvfu%H[#;,Kg'!AD2cXdf)_7;t1H[D78﻿L8—tOW!p&•tMpVQG,9“﻿$uf9 \n",
            "2\n",
            "bOjZ.C’F81*#WApT_V%?vj#\"zbGuImuclBcY88Sw™ia?Dr.8™ Q/ODb_z1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/divyansh070/paper_implemtaions.git\n",
        "%cd paper_implemtaions"
      ],
      "metadata": {
        "id": "v3LVKoqlB9si",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a6c0ab-7d62-477d-9096-0f99c508c912"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'paper_implemtaions'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/paper_implemtaions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioQbNSmDLWJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}